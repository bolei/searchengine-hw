package myimpl;

import java.io.File;
import java.io.IOException;
import java.io.StringReader;
import java.util.ArrayList;
import java.util.List;
import java.util.Map.Entry;

import org.apache.lucene.analysis.Analyzer.TokenStreamComponents;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import org.apache.lucene.document.Document;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.store.FSDirectory;
import org.apache.lucene.util.Version;

public class MiscUtil {

	private static IndexReader indexReader;
	public static MyEnglishAnalyzerConfigurable analyzer = new MyEnglishAnalyzerConfigurable(
			Version.LUCENE_43);
	static {
		analyzer.setLowercase(true);
		analyzer.setStopwordRemoval(true);
		analyzer.setStemmer(MyEnglishAnalyzerConfigurable.StemmerType.KSTEM);
	}

	public static IndexReader createIndexReader(String indexPath)
			throws IOException {
		indexReader = DirectoryReader.open(FSDirectory
				.open(new File(indexPath)));
		return indexReader;
	}

	public static IndexReader getIndexReader() {
		if (indexReader == null) {
			System.err.println("index reader is null");
			System.exit(1);
		}
		return indexReader;
	}

	/**
	 * Given a query string, returns the terms one at a time with stopwords
	 * removed and the terms stemmed using the Krovetz stemmer.
	 * 
	 * Use this method to process raw query terms.
	 * 
	 * @param query
	 *            String containing query
	 * @return Array of query tokens
	 * @throws IOException
	 */
	public static String[] tokenizeQuery(String query) throws IOException {

		TokenStreamComponents comp = analyzer.createComponents("dummy",
				new StringReader(query));
		TokenStream tokenStream = comp.getTokenStream();

		CharTermAttribute charTermAttribute = tokenStream
				.addAttribute(CharTermAttribute.class);
		tokenStream.reset();

		List<String> tokens = new ArrayList<String>();
		while (tokenStream.incrementToken()) {
			String term = charTermAttribute.toString();
			tokens.add(term);
		}
		return tokens.toArray(new String[tokens.size()]);
	}

	public static boolean isStopWord(String token) {
		return analyzer.getStopwordSet().contains(token);
	}

	/**
	 * Prints the query results.
	 * 
	 * THIS IS NOT THE CORRECT OUTPUT FORMAT. YOU MUST CHANGE THIS METHOD SO
	 * THAT IT OUTPUTS IN THE FORMAT SPECIFIED IN THE HOMEWORK PAGE, WHICH IS:
	 * 
	 * QueryID Q0 DocID Rank Score RunID
	 * 
	 * @param queryName
	 *            Original query.
	 * @param result
	 *            Result object generated by {@link Qryop#evaluate()}.
	 * @throws IOException
	 */
	public static void printResults(String queryId, MyQryResult result)
			throws IOException {

		if (!(result instanceof MyScoreList)) {
			System.out.println(queryId + ":\tNo results.");
			return;
		}
		MyScoreList sl = (MyScoreList) result;
		int rank = 1;
		for (Entry<Integer, Double> entry : sl.getSortedScores().entrySet()) {
			if (rank > 100) {
				return;
			}
			System.out.println(String.format("%s\tQ0\t%s\t%d\t%f\trun-1",
					queryId, getExternalDocid(entry.getKey()), rank,
					entry.getValue()));
			// System.out.println(String.format("%s\tQ0\t%s\t%d\t%f\trun-1",
			// queryId, entry.getKey(), rank, entry.getValue()));
			rank++;
		}
	}

	/**
	 * Get the external document id for a document specified by an internal
	 * document id. Ordinarily this would be a simple call to the Lucene index
	 * reader, but when the index was built, the indexer added "_0" to the end
	 * of each external document id. The correct solution would be to fix the
	 * index, but it's too late for that now, so it is fixed here before the id
	 * is returned.
	 * 
	 * @param iid
	 *            The internal document id of the document.
	 * @throws IOException
	 */
	private static String getExternalDocid(int iid) throws IOException {
		Document d = indexReader.document(iid);
		String eid = d.get("externalId");

		if ((eid != null) && eid.endsWith("_0"))
			eid = eid.substring(0, eid.length() - 2);

		return (eid);
	}
}
